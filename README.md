# Language Model Research Implementations
## Overview
This repository contains code implementations for illustrating concepts from two influential papers on language models and natural language understanding: "Language Models are Unsupervised Multitask Learners" and "Improving Language Understanding by Generative Pre-Training". These papers explore the capabilities of language models in learning a wide range of tasks without explicit supervision and enhancing language understanding through generative pre-training.

### Papers Summary
- Language Models are Unsupervised Multitask Learners: This paper discusses the potential of language models like GPT-2 in performing tasks like question answering and machine translation without specific task training.
- Improving Language Understanding by Generative Pre-Training: Focuses on a method that combines generative pre-training on a vast corpus of unlabeled text with fine-tuning on specific tasks, resulting in significant performance improvements in language understanding.

## Repository Content
- Training_GPT2.ipynb: Jupyter notebook containing code for training the GPT-2 model.
- Perplexity.ipynb: Jupyter notebook containing code for a radom evaluation of perplexity.
- Example_train.txt: Text file used for training the model.
- PDFs of both research papers for reference.
